//what makes an algorithm fast

//Space complexity - how much memory is used
//Time complexity - how many primitive operations are executed - input size and worst case seanario


//quadratic time, as the inputs increase `the number of operations increase n^2 (nested loops)


// 2 separate loops (finding the min and max numbers) 2n 

//f the list is already sorted the operation of finding min and max will be two, because there is no loops needed

//when its just one loop it will be n time complexity (n is the number of inputs) 


//O(n^2) is quadratci time (compares all the numbers)
//0(n) number of inputs times by the loop functions
//O(1) constant time, we know the   amount of operation regarless of the length of inputs

//> fast to slow
// constant, logarithmic, linear, quadratic, exponential
//O(1) O(logn) O(n) O(n^2) O(k^n) 

//logarithmic is the group when you dont know what to p[ut things in


//constant time operations . addition, pop (array) assigning a property to an object, greater than and less than

//unshift, shift are not constant time its linenar n time. 

//logarithmic time is when dealing with fractions. Like looping through an array and halfing the array. every time the input gets smaller and therefore less work needs to be done, base input is how uch you are dividing it by 

//nLogn linear but halfing the loop

//every time you look up an array its n, double nested loops n^2 , triple nested loops n^3 

//big O notation is used for time complexity 